#path to directory with .gguf files. It will be mounted with the same path inside containers
MODEL_DIR=

#configuration for llama.cpp build. should match the host CUDA version and ARCH
UBUNTU_VERSION=22.04
CUDA_VERSION=12.6.0
CUDA_DOCKER_ARCH=86

WEBUI_PORT=7777

#you can add more API external endpoints/keys for openwebUI to use models from. For example vllm for vision models etc. IT's optional
OPENAI_API_BASE_URLS=
OPENAI_API_KEYS=
MCPO_CONFIG_FILE=example-config.json
MCPO_API_KEY=key